{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'roberta.embeddings.position_ids', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing TFRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFRobertaModel were not initialized from the PyTorch model and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "382/382 [==============================] - 132s 327ms/step - loss: 0.6741 - accuracy: 0.7140\n",
      "Epoch 2/3\n",
      "382/382 [==============================] - 125s 328ms/step - loss: 0.4180 - accuracy: 0.8329\n",
      "Epoch 3/3\n",
      "382/382 [==============================] - 125s 327ms/step - loss: 0.2337 - accuracy: 0.9095\n",
      "96/96 [==============================] - 12s 114ms/step - loss: 0.2826 - accuracy: 0.8989\n",
      "Test Loss: 0.2825864553451538, Test Accuracy: 0.8988874554634094\n",
      "96/96 [==============================] - 12s 111ms/step\n",
      "F1 Score: 0.8980906188767069\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import RobertaTokenizer, TFRobertaModel\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 加载数据并预处理\n",
    "df = pd.read_csv('data.csv')\n",
    "df.dropna(inplace=True)  # 删除空值\n",
    "df['label'] = LabelEncoder().fit_transform(df['label'])  # 对标签进行编码\n",
    "\n",
    "# 使用 RoBERTa 分词器\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "roberta_model = TFRobertaModel.from_pretrained(\"roberta-base\")\n",
    "\n",
    "# 数据编码函数\n",
    "def encode_data(df, tokenizer, max_length=64):\n",
    "    encoded = tokenizer(\n",
    "        df['phrase'].tolist(),\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"tf\"\n",
    "    )\n",
    "    return encoded, df['label'].values\n",
    "\n",
    "# 编码数据\n",
    "encoded, y = encode_data(df, tokenizer)\n",
    "\n",
    "# 提取 input_ids 和 attention_mask 并转换为 NumPy 数组\n",
    "input_ids = encoded['input_ids'].numpy()\n",
    "attention_mask = encoded['attention_mask'].numpy()\n",
    "\n",
    "# 划分数据集为训练集和测试集\n",
    "train_input_ids, test_input_ids, train_attention_mask, test_attention_mask, train_y, test_y = train_test_split(\n",
    "    input_ids, attention_mask, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 创建训练数据集和测试数据集\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(({'input_ids': train_input_ids, 'attention_mask': train_attention_mask}, train_y)).batch(128)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(({'input_ids': test_input_ids, 'attention_mask': test_attention_mask}, test_y)).batch(128)\n",
    "\n",
    "# 定义类别数量和模型输入层\n",
    "num_classes = 3\n",
    "input_ids = Input(shape=(64,), dtype=\"int32\", name='input_ids')\n",
    "attention_mask = Input(shape=(64,), dtype=\"int32\", name='attention_mask')\n",
    "\n",
    "# 获取 RoBERTa 模型的输出\n",
    "bert_outputs = roberta_model([input_ids, attention_mask])\n",
    "bert_pooled_output = bert_outputs[1]  # 使用池化输出\n",
    "\n",
    "# 添加输出层并创建模型\n",
    "outputs = Dense(units=num_classes, activation=\"softmax\")(bert_pooled_output)\n",
    "model = Model(inputs=[input_ids, attention_mask], outputs=outputs)\n",
    "\n",
    "# 编译模型\n",
    "adam = Adam(learning_rate=2e-5, epsilon=1e-08)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"], optimizer=adam)\n",
    "\n",
    "# 训练模型\n",
    "model.fit(train_dataset, epochs=15)\n",
    "\n",
    "# 保存模型\n",
    "model.save('my_roberta_model.h5')\n",
    "\n",
    "# 评估模型\n",
    "results = model.evaluate(test_dataset)\n",
    "print(f\"Test Loss: {results[0]}, Test Accuracy: {results[1]}\")\n",
    "\n",
    "# 预测和计算 F1-score\n",
    "predictions = model.predict(test_dataset)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "f1 = f1_score(test_y, predicted_labels, average='macro')\n",
    "print(f\"F1 Score: {f1}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-11T16:33:49.264627Z",
     "start_time": "2024-05-11T16:26:52.532384600Z"
    }
   },
   "id": "f879f0dc21f825db"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "28cf19b17b379c3c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
